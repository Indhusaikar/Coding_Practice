{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "099daf7d-e011-4a37-bfd1-fa35fa56c166",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pathlib\n",
    "import cv2\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, classification_report, precision_score, recall_score, f1_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from keras.applications.mobilenet_v2 import MobileNetV2\n",
    "from keras.layers import GlobalAveragePooling2D\n",
    "from keras.models import Sequential\n",
    "import matplotlib.pyplot as plt  # For displaying images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c49d521-124a-40db-86be-19d2d232493c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define constants\n",
    "IMAGE_SHAPE = (200, 200)  # Resize all images to this shape for MobileNetV2\n",
    "BATCH_SIZE = 100\n",
    "TRAIN_DIR = 'F:/indhu/Datasets/bone fracture_ x-rays/train'  # Path to training dataset\n",
    "VAL_DIR = 'F:/indhu/Datasets/bone fracture_ x-rays/val'  # Path to validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "123b8208-5bea-4a18-ac5c-18d27723ffb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use TensorFlow Dataset to load images efficiently\n",
    "def load_data_from_directory(data_dir, batch_size=BATCH_SIZE):\n",
    "    data_dir = pathlib.Path(data_dir)\n",
    "    categories = os.listdir(data_dir)\n",
    "    label_dict = {category: index for index, category in enumerate(categories)}\n",
    "    \n",
    "    image_paths = []\n",
    "    labels = []\n",
    "    for category in categories:\n",
    "        category_path = data_dir / category\n",
    "        for image_path in pathlib.Path(category_path).glob('*'):\n",
    "            image_paths.append(str(image_path))\n",
    "            labels.append(label_dict[category])\n",
    "    \n",
    "    return image_paths, np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "27fc726c-8d1f-4d0e-b699-2f76619246ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load image paths and labels\n",
    "train_image_paths, y_train = load_data_from_directory(TRAIN_DIR)\n",
    "val_image_paths, y_val = load_data_from_directory(VAL_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd8aaf3f-41c0-40a6-9f78-5e581c7978db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load and preprocess images\n",
    "def load_and_preprocess_images(image_paths, batch_size=BATCH_SIZE):\n",
    "    image_batch = []\n",
    "    for i, image_path in enumerate(image_paths):\n",
    "        img = cv2.imread(image_path)\n",
    "        resized_img = cv2.resize(img, IMAGE_SHAPE)\n",
    "        normalized_img = resized_img / 255.0  # Normalize images\n",
    "        image_batch.append(normalized_img)\n",
    "        \n",
    "        if len(image_batch) == batch_size or i == len(image_paths) - 1:\n",
    "            yield np.array(image_batch)\n",
    "            image_batch = []  # Reset batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "77735041-9377-42e0-baf0-c0656b1d8061",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Indhuma\\AppData\\Local\\Temp\\ipykernel_11056\\485066812.py:2: UserWarning: `input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n",
      "  base_model_mobilenet = MobileNetV2(input_shape=IMAGE_SHAPE + (3,), include_top=False, weights='imagenet')\n"
     ]
    }
   ],
   "source": [
    "# Create MobileNetV2 feature extractor\n",
    "base_model_mobilenet = MobileNetV2(input_shape=IMAGE_SHAPE + (3,), include_top=False, weights='imagenet')\n",
    "base_model_mobilenet.trainable = False  # Freeze the convolutional base\n",
    "\n",
    "feature_extractor_mobilenet = Sequential([\n",
    "    base_model_mobilenet,\n",
    "    GlobalAveragePooling2D()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a5e92150-05e9-40cc-9040-e30910ea2b13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 565ms/step\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 155ms/step\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 157ms/step\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 154ms/step\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 161ms/step\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 154ms/step\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 149ms/step\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 158ms/step\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 158ms/step\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 163ms/step\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 155ms/step\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 151ms/step\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 158ms/step\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 154ms/step\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 154ms/step\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 158ms/step\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 160ms/step\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 157ms/step\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 157ms/step\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 155ms/step\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 154ms/step\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 151ms/step\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 156ms/step\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 154ms/step\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 154ms/step\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 154ms/step\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 159ms/step\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 159ms/step\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 158ms/step\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 154ms/step\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 157ms/step\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 166ms/step\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 157ms/step\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 156ms/step\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 157ms/step\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 162ms/step\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 155ms/step\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 157ms/step\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 156ms/step\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 154ms/step\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 156ms/step\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 156ms/step\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 158ms/step\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 159ms/step\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 160ms/step\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 164ms/step\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 159ms/step\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 159ms/step\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 160ms/step\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 156ms/step\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 157ms/step\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 156ms/step\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 157ms/step\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 158ms/step\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 163ms/step\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 161ms/step\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 159ms/step\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 157ms/step\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 161ms/step\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 159ms/step\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 164ms/step\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 161ms/step\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 158ms/step\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 154ms/step\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 159ms/step\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 165ms/step\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 160ms/step\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 164ms/step\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 173ms/step\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 179ms/step\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 165ms/step\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 157ms/step\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 169ms/step\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 156ms/step\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 165ms/step\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 156ms/step\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 156ms/step\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 160ms/step\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 150ms/step\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 153ms/step\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 158ms/step\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 155ms/step\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 153ms/step\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 158ms/step\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 160ms/step\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 190ms/step\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 195ms/step\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 189ms/step\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 267ms/step\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 183ms/step\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 178ms/step\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 159ms/step\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 152ms/step\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 153ms/step\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 150ms/step\n"
     ]
    }
   ],
   "source": [
    "# Extract features using MobileNetV2 (batch by batch)\n",
    "train_features = []\n",
    "for batch in load_and_preprocess_images(train_image_paths, batch_size=BATCH_SIZE):\n",
    "    features = feature_extractor_mobilenet.predict(batch)\n",
    "    train_features.append(features)\n",
    "\n",
    "train_features = np.concatenate(train_features, axis=0)  # Combine all batches\n",
    "\n",
    "val_features = []\n",
    "for batch in load_and_preprocess_images(val_image_paths, batch_size=BATCH_SIZE):\n",
    "    features = feature_extractor_mobilenet.predict(batch)\n",
    "    val_features.append(features)\n",
    "\n",
    "val_features = np.concatenate(val_features, axis=0)  # Combine all batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ed752344-f268-488a-b4ea-63750c542846",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize classifiers\n",
    "classifiers = {\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=200, random_state=42),\n",
    "    \"K-Nearest Neighbors\": KNeighborsClassifier(n_neighbors=5),\n",
    "    \"Support Vector Machine\": SVC(kernel='rbf', probability=True, random_state=42),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(random_state=42),\n",
    "    \"Naive Bayes\": GaussianNB()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9008cf84-fbfc-4071-b0a2-db299e76a8ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Random Forest...\n",
      "Random Forest Accuracy: 0.7217\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.71      0.75       360\n",
      "           1       0.63      0.75      0.68       240\n",
      "\n",
      "    accuracy                           0.72       600\n",
      "   macro avg       0.72      0.73      0.72       600\n",
      "weighted avg       0.74      0.72      0.72       600\n",
      "\n",
      "Evaluating Logistic Regression...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Indhuma\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy: 0.6583\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.66      0.70       360\n",
      "           1       0.56      0.66      0.61       240\n",
      "\n",
      "    accuracy                           0.66       600\n",
      "   macro avg       0.65      0.66      0.65       600\n",
      "weighted avg       0.67      0.66      0.66       600\n",
      "\n",
      "Evaluating K-Nearest Neighbors...\n",
      "K-Nearest Neighbors Accuracy: 0.7050\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.60      0.71       360\n",
      "           1       0.59      0.86      0.70       240\n",
      "\n",
      "    accuracy                           0.70       600\n",
      "   macro avg       0.73      0.73      0.70       600\n",
      "weighted avg       0.76      0.70      0.71       600\n",
      "\n",
      "Evaluating Support Vector Machine...\n",
      "Support Vector Machine Accuracy: 0.7183\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.75      0.76       360\n",
      "           1       0.64      0.67      0.66       240\n",
      "\n",
      "    accuracy                           0.72       600\n",
      "   macro avg       0.71      0.71      0.71       600\n",
      "weighted avg       0.72      0.72      0.72       600\n",
      "\n",
      "Evaluating Decision Tree...\n",
      "Decision Tree Accuracy: 0.6483\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.68      0.70       360\n",
      "           1       0.56      0.60      0.58       240\n",
      "\n",
      "    accuracy                           0.65       600\n",
      "   macro avg       0.64      0.64      0.64       600\n",
      "weighted avg       0.65      0.65      0.65       600\n",
      "\n",
      "Evaluating Naive Bayes...\n",
      "Naive Bayes Accuracy: 0.6217\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.41      0.57       360\n",
      "           1       0.51      0.94      0.66       240\n",
      "\n",
      "    accuracy                           0.62       600\n",
      "   macro avg       0.71      0.67      0.62       600\n",
      "weighted avg       0.75      0.62      0.61       600\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate classifiers using validation set\n",
    "for model_name, model in classifiers.items():\n",
    "    print(f\"Evaluating {model_name}...\")\n",
    "    model.fit(train_features, y_train)\n",
    "    y_pred = model.predict(val_features)\n",
    "    print(f\"{model_name} Accuracy: {accuracy_score(y_val, y_pred):.4f}\")\n",
    "    print(classification_report(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "27a41ecc-acb1-4b72-af53-c22b02ab3590",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K-Fold Cross Validation for Random Forest...\n",
      "Average Accuracy: 0.9753\n",
      "Average Precision: 0.9753\n",
      "Average Recall: 0.9753\n",
      "Average F1 Score: 0.9753\n",
      "\n",
      "K-Fold Cross Validation for Logistic Regression...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Indhuma\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Indhuma\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Indhuma\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Indhuma\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Accuracy: 0.9694\n",
      "Average Precision: 0.9695\n",
      "Average Recall: 0.9694\n",
      "Average F1 Score: 0.9694\n",
      "\n",
      "K-Fold Cross Validation for K-Nearest Neighbors...\n",
      "Average Accuracy: 0.9927\n",
      "Average Precision: 0.9927\n",
      "Average Recall: 0.9927\n",
      "Average F1 Score: 0.9927\n",
      "\n",
      "K-Fold Cross Validation for Support Vector Machine...\n",
      "Average Accuracy: 0.9903\n",
      "Average Precision: 0.9904\n",
      "Average Recall: 0.9903\n",
      "Average F1 Score: 0.9903\n",
      "\n",
      "K-Fold Cross Validation for Decision Tree...\n",
      "Average Accuracy: 0.8139\n",
      "Average Precision: 0.8144\n",
      "Average Recall: 0.8139\n",
      "Average F1 Score: 0.8139\n",
      "\n",
      "K-Fold Cross Validation for Naive Bayes...\n",
      "Average Accuracy: 0.7391\n",
      "Average Precision: 0.7575\n",
      "Average Recall: 0.7391\n",
      "Average F1 Score: 0.7350\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate classifiers using k-fold cross-validation\n",
    "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "for clf_name, clf in classifiers.items():\n",
    "    print(f\"K-Fold Cross Validation for {clf_name}...\")\n",
    "    precision_scores = []\n",
    "    recall_scores = []\n",
    "    f1_scores = []\n",
    "    accuracy_scores = []\n",
    "\n",
    "    for train_index, val_index in kf.split(train_features, y_train):\n",
    "        X_train_fold, X_val_fold = train_features[train_index], train_features[val_index]\n",
    "        y_train_fold, y_val_fold = y_train[train_index], y_train[val_index]\n",
    "\n",
    "        clf.fit(X_train_fold, y_train_fold)\n",
    "        y_val_pred = clf.predict(X_val_fold)\n",
    "\n",
    "        accuracy_scores.append(accuracy_score(y_val_fold, y_val_pred))\n",
    "        precision_scores.append(precision_score(y_val_fold, y_val_pred, average='weighted'))\n",
    "        recall_scores.append(recall_score(y_val_fold, y_val_pred, average='weighted'))\n",
    "        f1_scores.append(f1_score(y_val_fold, y_val_pred, average='weighted'))\n",
    "\n",
    "    print(f\"Average Accuracy: {np.mean(accuracy_scores):.4f}\")\n",
    "    print(f\"Average Precision: {np.mean(precision_scores):.4f}\")\n",
    "    print(f\"Average Recall: {np.mean(recall_scores):.4f}\")\n",
    "    print(f\"Average F1 Score: {np.mean(f1_scores):.4f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ab0718-ba96-4b5a-b741-769a8f723bdc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
